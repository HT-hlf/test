{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YU4o-jdVJiwn"
      },
      "source": [
        "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/williamyang1991/DualStyleGAN/blob/master/notebooks/inference_playground.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "copuRWF_Jiwr"
      },
      "source": [
        "code is mainly modified from [pixel2style2pixel](https://github.com/eladrich/pixel2style2pixel/blob/master/notebooks/inference_playground.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "Uuviq3qQkUFy"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = \"0\"\n",
        "os.chdir('../')\n",
        "CODE_DIR = 'DualStyleGAN'\n",
        "device = 'cuda'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def creat_dir(out_dir):\n",
        "    if not os.path.isdir(out_dir):\n",
        "        os.makedirs(out_dir)"
      ],
      "metadata": {
        "id": "vuMzcsxZGjlt"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "QQ6XEmlHlXbk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39494398-d17a-4a30-b2e0-4adfaf6a6f90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'DualStyleGAN' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/williamyang1991/DualStyleGAN.git $CODE_DIR"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
        "!sudo unzip ninja-linux.zip -d /usr/local/bin/\n",
        "!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force "
      ],
      "metadata": {
        "id": "tJ6nPQ7-KXOs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2fdee45b-bfd0-4051-e5db-51d6a00b6332"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-03-28 10:54:32--  https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n",
            "Resolving github.com (github.com)... 140.82.114.4\n",
            "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/1335132/d2f252e2-9801-11e7-9fbf-bc7b4e4b5c83?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220328%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220328T105433Z&X-Amz-Expires=300&X-Amz-Signature=942229f8967c0c1eaae55a9c6f62bcf59891cba11599153d50e854c08af9c4cb&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=1335132&response-content-disposition=attachment%3B%20filename%3Dninja-linux.zip&response-content-type=application%2Foctet-stream [following]\n",
            "--2022-03-28 10:54:33--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/1335132/d2f252e2-9801-11e7-9fbf-bc7b4e4b5c83?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20220328%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20220328T105433Z&X-Amz-Expires=300&X-Amz-Signature=942229f8967c0c1eaae55a9c6f62bcf59891cba11599153d50e854c08af9c4cb&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=1335132&response-content-disposition=attachment%3B%20filename%3Dninja-linux.zip&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 77854 (76K) [application/octet-stream]\n",
            "Saving to: ‘ninja-linux.zip.2’\n",
            "\n",
            "ninja-linux.zip.2   100%[===================>]  76.03K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2022-03-28 10:54:33 (5.98 MB/s) - ‘ninja-linux.zip.2’ saved [77854/77854]\n",
            "\n",
            "Archive:  ninja-linux.zip\n",
            "replace /usr/local/bin/ninja? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install faiss-cpu\n",
        "!pip install wget"
      ],
      "metadata": {
        "id": "Vs9bqANcMH8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23baccYQlU9E"
      },
      "outputs": [],
      "source": [
        "os.chdir(f'./{CODE_DIR}')\n",
        "MODEL_DIR = os.path.join(os.path.dirname(os.getcwd()), CODE_DIR, 'checkpoint')\n",
        "DATA_DIR = os.path.join(os.path.dirname(os.getcwd()), CODE_DIR, 'data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d13v7In0kTJn"
      },
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import sys\n",
        "sys.path.append(\".\")\n",
        "sys.path.append(\"..\")\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from util import save_image, load_image, visualize\n",
        "import argparse\n",
        "from argparse import Namespace\n",
        "from torchvision import transforms\n",
        "from torch.nn import functional as F\n",
        "import torchvision\n",
        "import matplotlib.pyplot as plt\n",
        "from model.dualstylegan import DualStyleGAN\n",
        "from model.sampler.icp import ICPTrainer\n",
        "from model.encoder.psp import pSp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRjtz6uLkTJs"
      },
      "source": [
        "## Step 1: Select Style Type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OU9zKS7QJiww"
      },
      "outputs": [],
      "source": [
        "style_types = ['cartoon', 'caricature', 'anime', 'arcane', 'comic', 'pixar', 'slamdunk']\n",
        "style_type = style_types[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4etDz82xkTJz"
      },
      "source": [
        "## Step 2: Download Pretrained Models \n",
        "As part of this repository, we provide pretrained models. We'll download the model and save them to the folder `../checkpoint/`."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(os.path.join(MODEL_DIR, style_type)):\n",
        "    os.makedirs(os.path.join(MODEL_DIR, style_type))"
      ],
      "metadata": {
        "id": "-lxoXxFSP80G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSnjlBZOkTJ0"
      },
      "outputs": [],
      "source": [
        "def get_download_model_command(file_id, file_name):\n",
        "    \"\"\" Get wget download command for downloading the desired model and save to directory ../checkpoint/. \"\"\"\n",
        "    current_directory = os.getcwd()\n",
        "    save_path = MODEL_DIR\n",
        "    url = r\"\"\"wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id={FILE_ID}' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id={FILE_ID}\" -O {SAVE_PATH}/{FILE_NAME} && rm -rf /tmp/cookies.txt\"\"\".format(FILE_ID=file_id, FILE_NAME=file_name, SAVE_PATH=save_path)\n",
        "    return url"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m4sjldFMkTJ5"
      },
      "outputs": [],
      "source": [
        "MODEL_PATHS = {\n",
        "    \"encoder\": {\"id\": \"1NgI4mPkboYvYw3MWcdUaQhkr0OWgs9ej\", \"name\": \"encoder.pt\"},\n",
        "    \"cartoon-G\": {\"id\": \"1exS9cSFkg8J4keKPmq2zYQYfJYC5FkwL\", \"name\": \"generator.pt\"},\n",
        "    \"cartoon-N\": {\"id\": \"1JSCdO0hx8Z5mi5Q5hI9HMFhLQKykFX5N\", \"name\": \"sampler.pt\"},\n",
        "    \"cartoon-S\": {\"id\": \"1ce9v69JyW_Dtf7NhbOkfpH77bS_RK0vB\", \"name\": \"refined_exstyle_code.npy\"},\n",
        "    \"caricature-G\": {\"id\": \"1BXfTiMlvow7LR7w8w0cNfqIl-q2z0Hgc\", \"name\": \"generator.pt\"},\n",
        "    \"caricature-N\": {\"id\": \"1eJSoaGD7X0VbHS47YLehZayhWDSZ4L2Q\", \"name\": \"sampler.pt\"},\n",
        "    \"caricature-S\": {\"id\": \"1-p1FMRzP_msqkjndRK_0JasTdwQKDsov\", \"name\": \"refined_exstyle_code.npy\"},\n",
        "    \"anime-G\": {\"id\": \"1BToWH-9kEZIx2r5yFkbjoMw0642usI6y\", \"name\": \"generator.pt\"},\n",
        "    \"anime-N\": {\"id\": \"19rLqx_s_SUdiROGnF_C6_uOiINiNZ7g2\", \"name\": \"sampler.pt\"},\n",
        "    \"anime-S\": {\"id\": \"17-f7KtrgaQcnZysAftPogeBwz5nOWYuM\", \"name\": \"refined_exstyle_code.npy\"},\n",
        "    \"arcane-G\": {\"id\": \"15l2O7NOUAKXikZ96XpD-4khtbRtEAg-Q\", \"name\": \"generator.pt\"},\n",
        "    \"arcane-N\": {\"id\": \"1fa7p9ZtzV8wcasPqCYWMVFpb4BatwQHg\", \"name\": \"sampler.pt\"},\n",
        "    \"arcane-S\": {\"id\": \"1z3Nfbir5rN4CrzatfcgQ8u-x4V44QCn1\", \"name\": \"exstyle_code.npy\"},\n",
        "    \"comic-G\": {\"id\": \"1_t8lf9lTJLnLXrzhm7kPTSuNDdiZnyqE\", \"name\": \"generator.pt\"},\n",
        "    \"comic-N\": {\"id\": \"1RXrJPodIn7lCzdb5BFc03kKqHEazaJ-S\", \"name\": \"sampler.pt\"},\n",
        "    \"comic-S\": {\"id\": \"1ZfQ5quFqijvK3hO6f-YDYJMqd-UuQtU-\", \"name\": \"exstyle_code.npy\"},\n",
        "    \"pixar-G\": {\"id\": \"1TgH7WojxiJXQfnCroSRYc7BgxvYH9i81\", \"name\": \"generator.pt\"},\n",
        "    \"pixar-N\": {\"id\": \"18e5AoQ8js4iuck7VgI3hM_caCX5lXlH_\", \"name\": \"sampler.pt\"},\n",
        "    \"pixar-S\": {\"id\": \"1I9mRTX2QnadSDDJIYM_ntyLrXjZoN7L-\", \"name\": \"exstyle_code.npy\"},    \n",
        "    \"slamdunk-G\": {\"id\": \"1MGGxSCtyf9399squ3l8bl0hXkf5YWYNz\", \"name\": \"generator.pt\"},\n",
        "    \"slamdunk-N\": {\"id\": \"1-_L7YVb48sLr_kPpOcn4dUq7Cv08WQuG\", \"name\": \"sampler.pt\"},\n",
        "    \"slamdunk-S\": {\"id\": \"1Dgh11ZeXS2XIV2eJZAExWMjogxi_m_C8\", \"name\": \"exstyle_code.npy\"},     \n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DezLLUI6Jiwy"
      },
      "outputs": [],
      "source": [
        "# download pSp encoder\n",
        "path = MODEL_PATHS[\"encoder\"]\n",
        "download_command = get_download_model_command(file_id=path[\"id\"], file_name=path[\"name\"])\n",
        "!{download_command}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jQ31J_m7kTJ8"
      },
      "outputs": [],
      "source": [
        "# download dualstylegan\n",
        "path = MODEL_PATHS[style_type+'-G']\n",
        "download_command = get_download_model_command(file_id=path[\"id\"], file_name=os.path.join(style_type, path[\"name\"]))\n",
        "!{download_command}\n",
        "# download sampler\n",
        "path = MODEL_PATHS[style_type+'-N']\n",
        "download_command = get_download_model_command(file_id=path[\"id\"], file_name=os.path.join(style_type, path[\"name\"]))\n",
        "!{download_command}\n",
        "# download extrinsic style code\n",
        "path = MODEL_PATHS[style_type+'-S']\n",
        "download_command = get_download_model_command(file_id=path[\"id\"], file_name=os.path.join(style_type, path[\"name\"]))\n",
        "!{download_command}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAWrUehTkTKJ"
      },
      "source": [
        "## Step 3: Load Pretrained Model\n",
        "We assume that you have downloaded all relevant models and placed them in the directory defined by the above dictionary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ie99_G5GJiwz"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize(256),\n",
        "        transforms.CenterCrop(256),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# load DualStyleGAN\n",
        "generator = DualStyleGAN(1024, 512, 8, 2, res_index=6)\n",
        "generator.eval()\n",
        "ckpt = torch.load(os.path.join(MODEL_DIR, style_type, 'generator.pt'), map_location=lambda storage, loc: storage)\n",
        "generator.load_state_dict(ckpt[\"g_ema\"])\n",
        "generator = generator.to(device)\n",
        "\n",
        "# load encoder\n",
        "model_path = os.path.join(MODEL_DIR, 'encoder.pt')\n",
        "ckpt = torch.load(model_path, map_location='cpu')\n",
        "opts = ckpt['opts']\n",
        "opts['checkpoint_path'] = model_path\n",
        "opts = Namespace(**opts)\n",
        "opts.device = device\n",
        "encoder = pSp(opts)\n",
        "encoder.eval()\n",
        "encoder = encoder.to(device)\n",
        "\n",
        "# load extrinsic style code\n",
        "exstyles = np.load(os.path.join(MODEL_DIR, style_type, MODEL_PATHS[style_type+'-S'][\"name\"]), allow_pickle='TRUE').item()\n",
        "\n",
        "# load sampler network\n",
        "icptc = ICPTrainer(np.empty([0,512*11]), 128)\n",
        "icpts = ICPTrainer(np.empty([0,512*7]), 128)\n",
        "ckpt = torch.load(os.path.join(MODEL_DIR, style_type, 'sampler.pt'), map_location=lambda storage, loc: storage)\n",
        "icptc.icp.netT.load_state_dict(ckpt['color'])\n",
        "icpts.icp.netT.load_state_dict(ckpt['structure'])\n",
        "icptc.icp.netT = icptc.icp.netT.to(device)\n",
        "icpts.icp.netT = icpts.icp.netT.to(device)\n",
        "\n",
        "print('Model successfully loaded!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4weLFoPbkTKZ"
      },
      "source": [
        "## Step 5: Visualize Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d66s1hyRJiw1"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2H9zFLJkTKa"
      },
      "outputs": [],
      "source": [
        "# image_path = './data/content/unsplash-rDEOVtE7vOs.jpg'\n",
        "# original_image = load_image(image_path)\n",
        "image_path = './data/content/1.jpg'\n",
        "original_image = load_image(image_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lbLKtl-kTKc"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10),dpi=30)\n",
        "visualize(original_image[0])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6oqf8JwzK0K"
      },
      "source": [
        "### Align Image\n",
        "\n",
        "Note: Our style transfer assumes the input has been pre-aligned.\n",
        "If the original image is not pre-aligned, please run the following alignment scripts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_9W-FzCPJiw2"
      },
      "outputs": [],
      "source": [
        "if_align_face = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJ9Ce1aYzmFF"
      },
      "outputs": [],
      "source": [
        "def run_alignment(image_path):\n",
        "    import dlib\n",
        "    from model.encoder.align_all_parallel import align_face\n",
        "    modelname = os.path.join(MODEL_DIR, 'shape_predictor_68_face_landmarks.dat')\n",
        "    if not os.path.exists(modelname):\n",
        "        import wget, bz2\n",
        "        wget.download('http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2', modelname+'.bz2')\n",
        "        zipfile = bz2.BZ2File(modelname+'.bz2')\n",
        "        data = zipfile.read()\n",
        "        open(modelname, 'wb').write(data) \n",
        "    predictor = dlib.shape_predictor(modelname)\n",
        "    aligned_image = align_face(filepath=image_path, predictor=predictor)\n",
        "    return aligned_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEn5bM2SJiw2"
      },
      "outputs": [],
      "source": [
        "if if_align_face:\n",
        "    I = transform(run_alignment(image_path)).unsqueeze(dim=0).to(device)\n",
        "else:\n",
        "    I = F.adaptive_avg_pool2d(load_image(image_path).to(device), 256)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUBAfodh5PaM"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,10),dpi=30)\n",
        "visualize(I[0].cpu())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0BmXzu1kTKg"
      },
      "source": [
        "## Step 6: Perform Inference -- Style Transfer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cjE-6yvJiw3"
      },
      "source": [
        "### Select style image\n",
        "\n",
        "Select the style id (the mapping between id and style image filename are defined [here](https://github.com/williamyang1991/DualStyleGAN/data_preparation/id_filename_list.txt))\n",
        "We assume that you have downloaded the dataset and placed them in `./data/STYLE_TYPE/images/train/`.\n",
        "If not, the style images will not be shown below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cD4pThI_Jiw3"
      },
      "outputs": [],
      "source": [
        "style_id = 183"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gcTzJChWJiw3"
      },
      "outputs": [],
      "source": [
        "# try to load the style image\n",
        "stylename = list(exstyles.keys())[style_id]\n",
        "stylepath = os.path.join(DATA_DIR, style_type, 'images/train', stylename)\n",
        "print('loading %s'%stylepath)\n",
        "if os.path.exists(stylepath):\n",
        "    S = load_image(stylepath)\n",
        "    plt.figure(figsize=(10,10),dpi=30)\n",
        "    visualize(S[0])\n",
        "    plt.show()\n",
        "else:\n",
        "    print('%s is not found'%stylename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKu6cdC1Jiw4"
      },
      "source": [
        "### Style transfer with and without color preservation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v5POMJ5YkTKl"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    img_rec, instyle = encoder(I, randomize_noise=False, return_latents=True, \n",
        "                            z_plus_latent=True, return_z_plus_latent=True, resize=False)    \n",
        "    img_rec = torch.clamp(img_rec.detach(), -1, 1)\n",
        "    \n",
        "    latent = torch.tensor(exstyles[stylename]).repeat(2,1,1).to(device)\n",
        "    # latent[0] for both color and structrue transfer and latent[1] for only structrue transfer\n",
        "    latent[1,7:18] = instyle[0,7:18]\n",
        "    exstyle = generator.generator.style(latent.reshape(latent.shape[0]*latent.shape[1], latent.shape[2])).reshape(latent.shape)\n",
        "    \n",
        "    img_gen, _ = generator([instyle.repeat(2,1,1)], exstyle, z_plus_latent=True, \n",
        "                           truncation=0.7, truncation_latent=0, use_res=True, interp_weights=[0.6]*7+[1]*11)\n",
        "    img_gen = torch.clamp(img_gen.detach(), -1, 1)\n",
        "    # deactivate color-related layers by setting w_c = 0\n",
        "    img_gen2, _ = generator([instyle], exstyle[0:1], z_plus_latent=True, \n",
        "                            truncation=0.7, truncation_latent=0, use_res=True, interp_weights=[0.6]*7+[0]*11)\n",
        "    img_gen2 = torch.clamp(img_gen2.detach(), -1, 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_sDLQc_Jiw4"
      },
      "source": [
        "### Visualize Results\n",
        "From left to right:\n",
        "1. **pSp recontructed content image**\n",
        "2. **style transfer result**: both color and strcture styles are transferred\n",
        "3. **structure transfer result**: preserve the color of the content image by replacing the extrinsic color codes with intrinsic color codes\n",
        "4. **structure transfer result**: preserve the color of the content image by deactivating color-related layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5IfHCsJcJiw4"
      },
      "outputs": [],
      "source": [
        "vis = torchvision.utils.make_grid(F.adaptive_avg_pool2d(torch.cat([img_rec, img_gen, img_gen2], dim=0), 256), 4, 1)\n",
        "plt.figure(figsize=(10,10),dpi=120)\n",
        "visualize(vis.cpu())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "style_types = ['anime', 'arcane', 'comic', 'pixar', 'slamdunk']\n",
        "style_nums =[174,100,101,122,120]\n",
        "if not os.path.exists(os.path.join(MODEL_DIR, style_type)):\n",
        "    os.makedirs(os.path.join(MODEL_DIR, style_type))\n",
        "def get_download_model_command(file_id, file_name):\n",
        "    \"\"\" Get wget download command for downloading the desired model and save to directory ../checkpoint/. \"\"\"\n",
        "    current_directory = os.getcwd()\n",
        "    save_path = MODEL_DIR\n",
        "    url = r\"\"\"wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id={FILE_ID}' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id={FILE_ID}\" -O {SAVE_PATH}/{FILE_NAME} && rm -rf /tmp/cookies.txt\"\"\".format(FILE_ID=file_id, FILE_NAME=file_name, SAVE_PATH=save_path)\n",
        "    return url\n",
        "MODEL_PATHS = {\n",
        "    \"encoder\": {\"id\": \"1NgI4mPkboYvYw3MWcdUaQhkr0OWgs9ej\", \"name\": \"encoder.pt\"},\n",
        "    \"cartoon-G\": {\"id\": \"1exS9cSFkg8J4keKPmq2zYQYfJYC5FkwL\", \"name\": \"generator.pt\"},\n",
        "    \"cartoon-N\": {\"id\": \"1JSCdO0hx8Z5mi5Q5hI9HMFhLQKykFX5N\", \"name\": \"sampler.pt\"},\n",
        "    \"cartoon-S\": {\"id\": \"1ce9v69JyW_Dtf7NhbOkfpH77bS_RK0vB\", \"name\": \"refined_exstyle_code.npy\"},\n",
        "    \"caricature-G\": {\"id\": \"1BXfTiMlvow7LR7w8w0cNfqIl-q2z0Hgc\", \"name\": \"generator.pt\"},\n",
        "    \"caricature-N\": {\"id\": \"1eJSoaGD7X0VbHS47YLehZayhWDSZ4L2Q\", \"name\": \"sampler.pt\"},\n",
        "    \"caricature-S\": {\"id\": \"1-p1FMRzP_msqkjndRK_0JasTdwQKDsov\", \"name\": \"refined_exstyle_code.npy\"},\n",
        "    \"anime-G\": {\"id\": \"1BToWH-9kEZIx2r5yFkbjoMw0642usI6y\", \"name\": \"generator.pt\"},\n",
        "    \"anime-N\": {\"id\": \"19rLqx_s_SUdiROGnF_C6_uOiINiNZ7g2\", \"name\": \"sampler.pt\"},\n",
        "    \"anime-S\": {\"id\": \"17-f7KtrgaQcnZysAftPogeBwz5nOWYuM\", \"name\": \"refined_exstyle_code.npy\"},\n",
        "    \"arcane-G\": {\"id\": \"15l2O7NOUAKXikZ96XpD-4khtbRtEAg-Q\", \"name\": \"generator.pt\"},\n",
        "    \"arcane-N\": {\"id\": \"1fa7p9ZtzV8wcasPqCYWMVFpb4BatwQHg\", \"name\": \"sampler.pt\"},\n",
        "    \"arcane-S\": {\"id\": \"1z3Nfbir5rN4CrzatfcgQ8u-x4V44QCn1\", \"name\": \"exstyle_code.npy\"},\n",
        "    \"comic-G\": {\"id\": \"1_t8lf9lTJLnLXrzhm7kPTSuNDdiZnyqE\", \"name\": \"generator.pt\"},\n",
        "    \"comic-N\": {\"id\": \"1RXrJPodIn7lCzdb5BFc03kKqHEazaJ-S\", \"name\": \"sampler.pt\"},\n",
        "    \"comic-S\": {\"id\": \"1ZfQ5quFqijvK3hO6f-YDYJMqd-UuQtU-\", \"name\": \"exstyle_code.npy\"},\n",
        "    \"pixar-G\": {\"id\": \"1TgH7WojxiJXQfnCroSRYc7BgxvYH9i81\", \"name\": \"generator.pt\"},\n",
        "    \"pixar-N\": {\"id\": \"18e5AoQ8js4iuck7VgI3hM_caCX5lXlH_\", \"name\": \"sampler.pt\"},\n",
        "    \"pixar-S\": {\"id\": \"1I9mRTX2QnadSDDJIYM_ntyLrXjZoN7L-\", \"name\": \"exstyle_code.npy\"},    \n",
        "    \"slamdunk-G\": {\"id\": \"1MGGxSCtyf9399squ3l8bl0hXkf5YWYNz\", \"name\": \"generator.pt\"},\n",
        "    \"slamdunk-N\": {\"id\": \"1-_L7YVb48sLr_kPpOcn4dUq7Cv08WQuG\", \"name\": \"sampler.pt\"},\n",
        "    \"slamdunk-S\": {\"id\": \"1Dgh11ZeXS2XIV2eJZAExWMjogxi_m_C8\", \"name\": \"exstyle_code.npy\"},     \n",
        "}\n",
        "# download pSp encoder\n",
        "path = MODEL_PATHS[\"encoder\"]\n",
        "download_command = get_download_model_command(file_id=path[\"id\"], file_name=path[\"name\"])\n",
        "!{download_command}\n",
        "\n",
        "# download dualstylegan\n",
        "path = MODEL_PATHS[style_type+'-G']\n",
        "download_command = get_download_model_command(file_id=path[\"id\"], file_name=os.path.join(style_type, path[\"name\"]))\n",
        "!{download_command}\n",
        "# download sampler\n",
        "path = MODEL_PATHS[style_type+'-N']\n",
        "download_command = get_download_model_command(file_id=path[\"id\"], file_name=os.path.join(style_type, path[\"name\"]))\n",
        "!{download_command}\n",
        "# download extrinsic style code\n",
        "path = MODEL_PATHS[style_type+'-S']\n",
        "download_command = get_download_model_command(file_id=path[\"id\"], file_name=os.path.join(style_type, path[\"name\"]))\n",
        "!{download_command}\n",
        "for m in range(5):\n",
        "  style_type = style_types[m]\n",
        "  style_num=style_nums[m]\n",
        "\n",
        "  transform = transforms.Compose(\n",
        "      [\n",
        "          transforms.Resize(256),\n",
        "          transforms.CenterCrop(256),\n",
        "          transforms.ToTensor(),\n",
        "          transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
        "      ]\n",
        "  )\n",
        "\n",
        "  # load DualStyleGAN\n",
        "  generator = DualStyleGAN(1024, 512, 8, 2, res_index=6)\n",
        "  generator.eval()\n",
        "  ckpt = torch.load(os.path.join(MODEL_DIR, style_type, 'generator.pt'), map_location=lambda storage, loc: storage)\n",
        "  generator.load_state_dict(ckpt[\"g_ema\"])\n",
        "  generator = generator.to(device)\n",
        "\n",
        "  # load encoder\n",
        "  model_path = os.path.join(MODEL_DIR, 'encoder.pt')\n",
        "  ckpt = torch.load(model_path, map_location='cpu')\n",
        "  opts = ckpt['opts']\n",
        "  opts['checkpoint_path'] = model_path\n",
        "  opts = Namespace(**opts)\n",
        "  opts.device = device\n",
        "  encoder = pSp(opts)\n",
        "  encoder.eval()\n",
        "  encoder = encoder.to(device)\n",
        "\n",
        "  # load extrinsic style code\n",
        "  exstyles = np.load(os.path.join(MODEL_DIR, style_type, MODEL_PATHS[style_type+'-S'][\"name\"]), allow_pickle='TRUE').item()\n",
        "\n",
        "  # load sampler network\n",
        "  icptc = ICPTrainer(np.empty([0,512*11]), 128)\n",
        "  icpts = ICPTrainer(np.empty([0,512*7]), 128)\n",
        "  ckpt = torch.load(os.path.join(MODEL_DIR, style_type, 'sampler.pt'), map_location=lambda storage, loc: storage)\n",
        "  icptc.icp.netT.load_state_dict(ckpt['color'])\n",
        "  icpts.icp.netT.load_state_dict(ckpt['structure'])\n",
        "  icptc.icp.netT = icptc.icp.netT.to(device)\n",
        "  icpts.icp.netT = icpts.icp.netT.to(device)\n",
        "\n",
        "  print('Model successfully loaded!')\n",
        "  # image_path = './data/content/unsplash-rDEOVtE7vOs.jpg'\n",
        "  # original_image = load_image(image_path)\n",
        "  image_path = './data/content/2.jpg'\n",
        "  original_image = load_image(image_path)\n",
        "\n",
        "\n",
        "\n",
        "  if if_align_face:\n",
        "      I = transform(run_alignment(image_path)).unsqueeze(dim=0).to(device)\n",
        "  else:\n",
        "      I = F.adaptive_avg_pool2d(load_image(image_path).to(device), 256)\n",
        "  for k in range(style_num):\n",
        "    #0~316\n",
        "    style_id = k\n",
        "    # try to load the style image\n",
        "    stylename = list(exstyles.keys())[style_id]\n",
        "    stylepath = os.path.join(DATA_DIR, style_type, 'images/train', stylename)\n",
        "    print('loading %s'%stylepath)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        img_rec, instyle = encoder(I, randomize_noise=False, return_latents=True, \n",
        "                                z_plus_latent=True, return_z_plus_latent=True, resize=False)    \n",
        "        img_rec = torch.clamp(img_rec.detach(), -1, 1)\n",
        "        \n",
        "        latent = torch.tensor(exstyles[stylename]).repeat(2,1,1).to(device)\n",
        "        # latent[0] for both color and structrue transfer and latent[1] for only structrue transfer\n",
        "        latent[1,7:18] = instyle[0,7:18]\n",
        "        exstyle = generator.generator.style(latent.reshape(latent.shape[0]*latent.shape[1], latent.shape[2])).reshape(latent.shape)\n",
        "        \n",
        "        img_gen, _ = generator([instyle.repeat(2,1,1)], exstyle, z_plus_latent=True, \n",
        "                              truncation=0.7, truncation_latent=0, use_res=True, interp_weights=[0.6]*7+[1]*11)\n",
        "        img_gen = torch.clamp(img_gen.detach(), -1, 1)\n",
        "        # deactivate color-related layers by setting w_c = 0\n",
        "        img_gen2, _ = generator([instyle], exstyle[0:1], z_plus_latent=True, \n",
        "                                truncation=0.7, truncation_latent=0, use_res=True, interp_weights=[0.6]*7+[0]*11)\n",
        "        img_gen2 = torch.clamp(img_gen2.detach(), -1, 1)\n",
        "    results = []\n",
        "    for i in range(6): # change weights of structure codes \n",
        "        for j in range(6): # change weights of color codes\n",
        "            w = [i/5.0]*7+[j/5.0]*11\n",
        "\n",
        "            img_gen, _ = generator([instyle], exstyle[0:1], z_plus_latent=True, \n",
        "                                    truncation=0.7, truncation_latent=0, use_res=True, interp_weights=w)\n",
        "            img_gen = torch.clamp(F.adaptive_avg_pool2d(img_gen.detach(), 128), -1, 1)\n",
        "            results += [img_gen]\n",
        "            # print(img_gen.shape)\n",
        "            img_gen_ht=img_gen[0].clone()\n",
        "            # print(img_gen_ht.data)\n",
        "            path_1='ht/'+str(m)+'_'+str(k)+'_'+str(i)+'_'+str(j)+'.jpg'\n",
        "            img_gen_ht=img_gen_ht.transpose(0,1)\n",
        "            img_gen_ht=img_gen_ht.transpose(1,2)\n",
        "            img_gen_ht=img_gen_ht.cpu().numpy()\n",
        "            heatmap_max = np.max(img_gen_ht)\n",
        "            heatmap_min = np.min(img_gen_ht)\n",
        "\n",
        "            # heatmap = np.maximum(heatmap, 0)\n",
        "            img_gen_ht = (img_gen_ht-heatmap_min) /(heatmap_max-heatmap_min)*255\n",
        "            img_gen_ht=img_gen_ht[:,:,::-1]\n",
        "            # print(img_gen_ht.shape)\n",
        "            cv2.imwrite(path_1,img_gen_ht)\n",
        "            # plt.subplots_adjust(left=None, bottom=None, right=None, top=None,wspace=None, hspace=None)  # 调整子图间距\n",
        "            # plt.figure(figsize=(features_channel_sqrt*10, features_channel_sqrt*10))\n",
        "            # save_path_sum=save_path+'.jpg'\n",
        "            # plt.savefig(path_1)\n",
        "            \n",
        "    vis = torchvision.utils.make_grid(torch.cat(results, dim=0), 6, 1)\n",
        "    plt.figure(figsize=(10,10),dpi=120)\n",
        "    visualize(vis.cpu())\n",
        "    path_2='ht1/'+str(m)+'_'+str(k)+'.jpg'\n",
        "    plt.savefig(path_2)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "bjzXWysbiGeK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "efwgpchglxZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "q6KislKBl04p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r ht_2.zip ht_2/\n",
        "!zip -r ht1_2.zip ht1_2/\n",
        "!zip -r ht.zip ht/\n",
        "!zip -r ht1.zip ht1/"
      ],
      "metadata": {
        "id": "oQEMxyA9l1TB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ht/"
      ],
      "metadata": {
        "id": "XOcfIUYrpL4i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEsuQSS3Jiw4"
      },
      "source": [
        "### Navigation with different interp_weights to achieve flexible style manipulation\n",
        "\n",
        "Users are suggested to try different interp_weights to find the ideal results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnRD27bZJiw4"
      },
      "outputs": [],
      "source": [
        "results = []\n",
        "for i in range(6): # change weights of structure codes \n",
        "    for j in range(6): # change weights of color codes\n",
        "        w = [i/5.0]*7+[j/5.0]*11\n",
        "\n",
        "        img_gen, _ = generator([instyle], exstyle[0:1], z_plus_latent=True, \n",
        "                                truncation=0.7, truncation_latent=0, use_res=True, interp_weights=w)\n",
        "        img_gen = torch.clamp(F.adaptive_avg_pool2d(img_gen.detach(), 128), -1, 1)\n",
        "        results += [img_gen]\n",
        "        \n",
        "vis = torchvision.utils.make_grid(torch.cat(results, dim=0), 6, 1)\n",
        "plt.figure(figsize=(10,10),dpi=120)\n",
        "visualize(vis.cpu())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VkWJzjQ2Jiw5"
      },
      "source": [
        "Style fusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aetESvV0Jiw5"
      },
      "outputs": [],
      "source": [
        "style_id2 = 299\n",
        "# try to load the style image\n",
        "stylename2 = list(exstyles.keys())[style_id2]\n",
        "stylepath = os.path.join(DATA_DIR, style_type, 'images/train', stylename2)\n",
        "print('loading %s'%stylepath)\n",
        "if os.path.exists(stylepath):\n",
        "    S = load_image(stylepath)\n",
        "    plt.figure(figsize=(10,10),dpi=30)\n",
        "    visualize(S[0])\n",
        "    plt.show()\n",
        "else:\n",
        "    print('%s is not found'%stylename2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqLVInF9Jiw5"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    latent = torch.tensor(exstyles[stylename]).repeat(6,1,1).to(device)\n",
        "    latent2 = torch.tensor(exstyles[stylename2]).repeat(6,1,1).to(device)\n",
        "    fuse_weight = torch.arange(6).reshape(6,1,1).to(device) / 5.0\n",
        "    fuse_latent = latent * fuse_weight + latent2 * (1-fuse_weight)\n",
        "    exstyle = generator.generator.style(fuse_latent.reshape(fuse_latent.shape[0]*fuse_latent.shape[1], fuse_latent.shape[2])).reshape(fuse_latent.shape)\n",
        "    \n",
        "    img_gen, _ = generator([instyle.repeat(6,1,1)], exstyle, z_plus_latent=True, \n",
        "                           truncation=0.7, truncation_latent=0, use_res=True, interp_weights=[0.6]*7+[1]*11)\n",
        "    img_gen = F.adaptive_avg_pool2d(torch.clamp(img_gen.detach(), -1, 1), 128)\n",
        "    # print(img_gen.shape)\n",
        "    \n",
        "vis = torchvision.utils.make_grid(img_gen, 6, 1)\n",
        "plt.figure(figsize=(10,10),dpi=120)\n",
        "visualize(vis.cpu())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKn4Yg5VJiw5"
      },
      "source": [
        "## Step 7: Perform Inference -- Artistic Portrait Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0bB4I1wJiw6"
      },
      "source": [
        "### Randomly sample both intrinsic and extrinsic style codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7qds-vZJiw6"
      },
      "outputs": [],
      "source": [
        "seed = 123\n",
        "torch.manual_seed(seed)\n",
        "batch = 6 # sample 6 style codes\n",
        "\n",
        "with torch.no_grad():\n",
        "    instyle = torch.randn(6, 512).to(device)\n",
        "    # sample structure codes\n",
        "    res_in = icpts.icp.netT(torch.randn(batch, 128).to(device)).reshape(-1,7,512)\n",
        "    # sample color codes\n",
        "    ada_in = icptc.icp.netT(torch.randn(batch, 128).to(device)).reshape(-1,11,512)\n",
        "\n",
        "    # concatenate two codes to form the complete extrinsic style code\n",
        "    latent = torch.cat((res_in, ada_in), dim=1)\n",
        "    # map into W+ space\n",
        "    exstyle = generator.generator.style(latent.reshape(latent.shape[0]*latent.shape[1], latent.shape[2])).reshape(latent.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SkRL4FbUJiw6"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    img_gen, _ = generator([instyle], exstyle, input_is_latent=False, truncation=0.7, truncation_latent=0, \n",
        "                           use_res=True, interp_weights=[0.6]*7+[1]*11)\n",
        "    \n",
        "    img_gen = F.adaptive_avg_pool2d(torch.clamp(img_gen.detach(), -1, 1), 128)\n",
        "    \n",
        "vis = torchvision.utils.make_grid(img_gen, batch, 1)\n",
        "plt.figure(figsize=(10,10),dpi=120)\n",
        "visualize(vis.cpu())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pbT69tDJiw6"
      },
      "source": [
        "### Fix extrinsic style codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WCo5setJiw6"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    img_gen, _ = generator([instyle], exstyle[4:5].repeat(batch, 1, 1), input_is_latent=False, truncation=0.7, truncation_latent=0, \n",
        "                           use_res=True, interp_weights=[0.6]*7+[1]*11)\n",
        "    \n",
        "    img_gen = F.adaptive_avg_pool2d(torch.clamp(img_gen.detach(), -1, 1), 128)\n",
        "    \n",
        "vis = torchvision.utils.make_grid(img_gen, batch, 1)\n",
        "plt.figure(figsize=(10,10),dpi=120)\n",
        "visualize(vis.cpu())\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6SYmYE1Jiw6"
      },
      "source": [
        "### Fix intrinsic style codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OokWevo8Jiw7"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    img_gen, _ = generator([instyle[4:5].repeat(batch,1)], exstyle, input_is_latent=False, truncation=0.7, truncation_latent=0, \n",
        "                           use_res=True, interp_weights=[0.6]*7+[1]*11)\n",
        "    \n",
        "    img_gen = F.adaptive_avg_pool2d(torch.clamp(img_gen.detach(), -1, 1), 128)\n",
        "    \n",
        "vis = torchvision.utils.make_grid(img_gen, batch, 1)\n",
        "plt.figure(figsize=(10,10),dpi=120)\n",
        "visualize(vis.cpu())\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "inference_playground.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}